{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff89e15-c73a-4beb-8715-1550a905d54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.29236161708831787, 'token': 4193, 'token_str': '爱', 'sequence': '生 活 的 真 谛 是 爱 。'}, {'score': 0.06636393070220947, 'token': 5483, 'token_str': '美', 'sequence': '生 活 的 真 谛 是 美 。'}, {'score': 0.03497227281332016, 'token': 6613, 'token_str': '谁', 'sequence': '生 活 的 真 谛 是 谁 。'}, {'score': 0.03312210366129875, 'token': 455, 'token_str': '你', 'sequence': '生 活 的 真 谛 是 你 。'}, {'score': 0.0232483372092247, 'token': 1403, 'token_str': '在', 'sequence': '生 活 的 真 谛 是 在 。'}, {'score': 0.022972118109464645, 'token': 6572, 'token_str': '诗', 'sequence': '生 活 的 真 谛 是 诗 。'}, {'score': 0.017687523737549782, 'token': 1208, 'token_str': '啥', 'sequence': '生 活 的 真 谛 是 啥 。'}, {'score': 0.016404232010245323, 'token': 1832, 'token_str': '它', 'sequence': '生 活 的 真 谛 是 它 。'}, {'score': 0.015613644383847713, 'token': 4729, 'token_str': '真', 'sequence': '生 活 的 真 谛 是 真 。'}, {'score': 0.014251182787120342, 'token': 371, 'token_str': '他', 'sequence': '生 活 的 真 谛 是 他 。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-Chinese', use_fast=False)\n",
    "model=AutoModelForMaskedLM.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-Chinese')\n",
    "text = '生活的真谛是[MASK]。'\n",
    "fillmask_pipe = FillMaskPipeline(model, tokenizer, device=0)\n",
    "print(fillmask_pipe(text, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4198a156-7f9a-4691-9d33-91e205175af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese' is the correct path to a directory containing all relevant files for a DebertaV2Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m=\u001b[39mAutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m生活的真谛是[MASK]。\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/work/_p310/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:679\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         )\n",
      "File \u001b[0;32m~/work/_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1789\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese' is the correct path to a directory containing all relevant files for a DebertaV2Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese', use_fast=False)\n",
    "model=AutoModelForMaskedLM.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-97M-CWS-Chinese')\n",
    "text = '生活的真谛是[MASK]。'\n",
    "fillmask_pipe = FillMaskPipeline(model, tokenizer, device=0)\n",
    "print(fillmask_pipe(text, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2591c70-6858-407c-b7e0-0b591ab21c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5303832292556763, 'token': 166, 'token_str': '北京', 'sequence': '中国首都位于北京 。'}, {'score': 0.23851409554481506, 'token': 119214, 'token_str': '首都北京', 'sequence': '中国首都位于首都北京 。'}, {'score': 0.05661068856716156, 'token': 2317, 'token_str': '北京', 'sequence': '中国首都位于 北京 。'}, {'score': 0.02194885164499283, 'token': 2922, 'token_str': '北京市', 'sequence': '中国首都位于北京市 。'}, {'score': 0.01590009778738022, 'token': 55805, 'token_str': '北京市朝阳区', 'sequence': '中国首都位于北京市朝阳区 。'}, {'score': 0.006819294765591621, 'token': 2071, 'token_str': '西安', 'sequence': '中国首都位于西安 。'}, {'score': 0.005462943576276302, 'token': 4361, 'token_str': '在北京', 'sequence': '中国首都位于在北京 。'}, {'score': 0.005386651027947664, 'token': 1858, 'token_str': '天津', 'sequence': '中国首都位于天津 。'}, {'score': 0.004599474370479584, 'token': 1386, 'token_str': '广州', 'sequence': '中国首都位于广州 。'}, {'score': 0.004495302215218544, 'token': 1418, 'token_str': '南京', 'sequence': '中国首都位于南京 。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-186M-Chinese-SentencePiece', use_fast=False)\n",
    "model=AutoModelForMaskedLM.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-186M-Chinese-SentencePiece')\n",
    "text = '中国首都位于<mask>。'\n",
    "fillmask_pipe = FillMaskPipeline(model, tokenizer)\n",
    "print(fillmask_pipe(text, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3f03e7-3ca1-400d-8326-03ce07562a60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9770157933235168, 'token': 3962, 'token_str': '漓', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 漓 江 。'}, {'score': 0.006565024610608816, 'token': 256, 'token_str': '两', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 两 江 。'}, {'score': 0.002794185420498252, 'token': 234, 'token_str': '三', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 三 江 。'}, {'score': 0.0017296758014708757, 'token': 3390, 'token_str': '榕', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 榕 江 。'}, {'score': 0.0012336598010733724, 'token': 3215, 'token_str': '柳', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 柳 江 。'}, {'score': 0.0010069729760289192, 'token': 6458, 'token_str': '西', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 西 江 。'}, {'score': 0.0009906459599733353, 'token': 6676, 'token_str': '象', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 象 江 。'}, {'score': 0.0008100103586912155, 'token': 3260, 'token_str': '桂', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 桂 江 。'}, {'score': 0.0003188226546626538, 'token': 8220, 'token_str': '龙', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 龙 江 。'}, {'score': 0.0003103285562247038, 'token': 2996, 'token_str': '春', 'sequence': '桂 林 是 世 界 闻 名 的 旅 游 城 市, 它 有 春 江 。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese', use_fast=False)\n",
    "model=AutoModelForMaskedLM.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese')\n",
    "text = '桂林是世界闻名的旅游城市,它有[MASK]江。'\n",
    "fillmask_pipe = FillMaskPipeline(model, tokenizer, device=0)\n",
    "print(fillmask_pipe(text, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b16f6e-bceb-4612-8bd8-85aeed8ae5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.35654008388519287, 'token': 4193, 'token_str': '爱', 'sequence': '生 活 的 真 谛 是 爱 。'}, {'score': 0.04206376522779465, 'token': 1208, 'token_str': '啥', 'sequence': '生 活 的 真 谛 是 啥 。'}, {'score': 0.026225335896015167, 'token': 287, 'token_str': '乐', 'sequence': '生 活 的 真 谛 是 乐 。'}, {'score': 0.02600248157978058, 'token': 353, 'token_str': '什', 'sequence': '生 活 的 真 谛 是 什 。'}, {'score': 0.02101045288145542, 'token': 5483, 'token_str': '美', 'sequence': '生 活 的 真 谛 是 美 。'}, {'score': 0.015401296317577362, 'token': 143, 'token_str': '”', 'sequence': '生 活 的 真 谛 是 ” 。'}, {'score': 0.014841888099908829, 'token': 280, 'token_str': '么', 'sequence': '生 活 的 真 谛 是 么 。'}, {'score': 0.012397834099829197, 'token': 4660, 'token_str': '的', 'sequence': '生 活 的 真 谛 是 的 。'}, {'score': 0.01212347112596035, 'token': 3503, 'token_str': '死', 'sequence': '生 活 的 真 谛 是 死 。'}, {'score': 0.01206271629780531, 'token': 3736, 'token_str': '活', 'sequence': '生 活 的 真 谛 是 活 。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-710M-Chinese', use_fast=False)\n",
    "model=AutoModelForMaskedLM.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-710M-Chinese')\n",
    "text = '生活的真谛是[MASK]。'\n",
    "fillmask_pipe = FillMaskPipeline(model, tokenizer, device=-1)\n",
    "print(fillmask_pipe(text, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4f9747-1ece-4402-9d97-ef40440ed59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fengshen import LongformerModel    \n",
    "from fengshen import LongformerConfig\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-Longformer-110M\")\n",
    "config = LongformerConfig.from_pretrained(\"IDEA-CCNL/Erlangshen-Longformer-110M\")\n",
    "model = LongformerModel.from_pretrained(\"IDEA-CCNL/Erlangshen-Longformer-110M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e1a997-ef83-464e-9314-01f20c69c13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fengshen import LongformerModel    \n",
    "from fengshen import LongformerConfig\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-Longformer-330M\")\n",
    "config = LongformerConfig.from_pretrained(\"IDEA-CCNL/Erlangshen-Longformer-330M\")\n",
    "model = LongformerModel.from_pretrained(\"IDEA-CCNL/Erlangshen-Longformer-330M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b2538f-5fee-4e68-89ae-bf60db2aa690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7303e-05, 9.8710e-01, 1.2873e-02]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-NLI')\n",
    "model=AutoModelForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-NLI')\n",
    "texta='今天的饭不好吃'\n",
    "textb='今天心情不好'\n",
    "output=model(torch.tensor([tokenizer.encode(texta,textb)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72503680-47f2-4c0d-8ce4-953fc5001861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9961, 0.0039]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Sentiment')\n",
    "model=AutoModelForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Sentiment')\n",
    "\n",
    "text='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(text)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "891c0c74-bd2e-40cd-9968-92596b5b3eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 1.5906e-06]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Similarity')\n",
    "model=AutoModelForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Similarity')\n",
    "\n",
    "texta='今天的饭不好吃'\n",
    "textb='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(texta,textb)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4a2f6cd-1f55-48f0-aa0c-8b69a494879c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5556007027626038, 'token': 4263, 'token_str': '爱', 'sequence': '生 活 的 真 谛 是 爱 。'}, {'score': 0.1618439108133316, 'token': 5401, 'token_str': '美', 'sequence': '生 活 的 真 谛 是 美 。'}, {'score': 0.04889656603336334, 'token': 4696, 'token_str': '真', 'sequence': '生 活 的 真 谛 是 真 。'}, {'score': 0.014448174275457859, 'token': 2640, 'token_str': '悟', 'sequence': '生 活 的 真 谛 是 悟 。'}, {'score': 0.014227446168661118, 'token': 4381, 'token_str': '玩', 'sequence': '生 活 的 真 谛 是 玩 。'}, {'score': 0.013059886172413826, 'token': 5010, 'token_str': '笑', 'sequence': '生 活 的 真 谛 是 笑 。'}, {'score': 0.01263718493282795, 'token': 782, 'token_str': '人', 'sequence': '生 活 的 真 谛 是 人 。'}, {'score': 0.011019867844879627, 'token': 1587, 'token_str': '善', 'sequence': '生 活 的 真 谛 是 善 。'}, {'score': 0.008693074807524681, 'token': 727, 'token_str': '乐', 'sequence': '生 活 的 真 谛 是 乐 。'}, {'score': 0.0063280267640948296, 'token': 2658, 'token_str': '情', 'sequence': '生 活 的 真 谛 是 情 。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-3.9B-Chinese', use_fast=False)\n",
    "model=AutoModelForMaskedLM.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-3.9B-Chinese')\n",
    "text = '生活的真谛是[MASK]。'\n",
    "fillmask_pipe = FillMaskPipeline(model, tokenizer)\n",
    "print(fillmask_pipe(text, top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f26891a-0adb-4c6b-9f73-e766eeb430f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0020, 0.1467, 0.8513]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-NLI')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-NLI')\n",
    "\n",
    "texta='今天的饭不好吃'\n",
    "textb='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(texta,textb)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc1ae32-2389-4ace-8fda-6cd452dc3c73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9551, 0.0449]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment')\n",
    "\n",
    "text='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(text)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d86c6fc-49b1-4cdf-be97-7c0a0b6ec2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9978e-01, 2.2166e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Similarity')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Similarity')\n",
    "\n",
    "texta='今天的饭不好吃'\n",
    "textb='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(texta,textb)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68d2166-7635-45e3-bcb5-056eb1dabae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1931e-04, 9.9940e-01, 2.8100e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-NLI')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-NLI')\n",
    "\n",
    "texta='今天的饭不好吃'\n",
    "textb='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(texta,textb)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe9766ca-009f-48cd-9348-a70b190a397f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9937e-01, 6.3242e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment')\n",
    "\n",
    "text='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(text)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f4aba2-9596-4a67-9420-0471fab23d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9989e-01, 1.0849e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Similarity')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Similarity')\n",
    "\n",
    "texta='今天的饭不好吃'\n",
    "textb='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(texta,textb)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7b092ef-ac07-4d26-ba23-01b900c96645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UbertPiplines' from 'fengshen' (/home/sha/src/Fengshenbang-LM/fengshen/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UbertPiplines\n\u001b[1;32m      4\u001b[0m total_parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTASK NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m total_parser \u001b[38;5;241m=\u001b[39m UbertPiplines\u001b[38;5;241m.\u001b[39mpiplines_args(total_parser)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UbertPiplines' from 'fengshen' (/home/sha/src/Fengshenbang-LM/fengshen/__init__.py)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from fengshen import UbertPiplines\n",
    "\n",
    "total_parser = argparse.ArgumentParser(\"TASK NAME\")\n",
    "total_parser = UbertPiplines.piplines_args(total_parser)\n",
    "args = total_parser.parse_args()\n",
    "args.pretrained_model_path = 'IDEA-CCNL/Erlangshen-Ubert-110M-Chinese'  #预训练模型路径\n",
    "test_data=[\n",
    "    {\n",
    "        \"task_type\": \"抽取任务\", \n",
    "        \"subtask_type\": \"实体识别\", \n",
    "        \"text\": \"这也让很多业主据此认为，雅清苑是政府公务员挤对了国家的经适房政策。\", \n",
    "        \"choices\": [ \n",
    "            {\"entity_type\": \"小区名字\"}, \n",
    "            {\"entity_type\": \"岗位职责\"}\n",
    "            ],\n",
    "        \"id\": 0}\n",
    "]\n",
    "\n",
    "model = UbertPiplines(args)\n",
    "result = model.predict(test_data)\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f05b6575-7909-449a-b2db-2616be86f456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UbertPiplines' from 'fengshen' (/home/sha/src/Fengshenbang-LM/fengshen/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UbertPiplines\n\u001b[1;32m      4\u001b[0m total_parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTASK NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m total_parser \u001b[38;5;241m=\u001b[39m UbertPiplines\u001b[38;5;241m.\u001b[39mpiplines_args(total_parser)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UbertPiplines' from 'fengshen' (/home/sha/src/Fengshenbang-LM/fengshen/__init__.py)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from fengshen import UbertPiplines\n",
    "\n",
    "total_parser = argparse.ArgumentParser(\"TASK NAME\")\n",
    "total_parser = UbertPiplines.piplines_args(total_parser)\n",
    "args = total_parser.parse_args()\n",
    "\n",
    "args.pretrained_model_path = \"IDEA-CCNL/Erlangshen-Ubert-330M-Chinese\"\n",
    "\n",
    "test_data=[\n",
    "    {\n",
    "        \"task_type\": \"抽取任务\", \n",
    "        \"subtask_type\": \"实体识别\", \n",
    "        \"text\": \"这也让很多业主据此认为，雅清苑是政府公务员挤对了国家的经适房政策。\", \n",
    "        \"choices\": [ \n",
    "            {\"entity_type\": \"小区名字\"}, \n",
    "            {\"entity_type\": \"岗位职责\"}\n",
    "            ],\n",
    "        \"id\": 0}\n",
    "]\n",
    "\n",
    "model = UbertPiplines(args)\n",
    "result = model.predict(test_data)\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adc6de4f-ce41-4aa8-8333-4784f3ccca3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cached_path' from 'transformers' (/home/sha/work/_p310/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzen1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mngram_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenNgramDict\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzen1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzen1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenForSequenceClassification, ZenForTokenClassification\n",
      "File \u001b[0;32m~/src/Fengshenbang-LM/fengshen/models/zen1/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mngram_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenNgramDict, NGRAM_DICT_NAME\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenConfig, ZenModel, ZenForPreTraining, ZenForTokenClassification, ZenForSequenceClassification\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BasicTokenizer, WordpieceTokenizer\n",
      "File \u001b[0;32m~/src/Fengshenbang-LM/fengshen/models/zen1/ngram_utils.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_path\n\u001b[1;32m     22\u001b[0m NGRAM_DICT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngram.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_path' from 'transformers' (/home/sha/work/_p310/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from fengshen.models.zen1.ngram_utils import ZenNgramDict\n",
    "from fengshen.models.zen1.tokenization import BertTokenizer\n",
    "from fengshen.models.zen1.modeling import ZenForSequenceClassification, ZenForTokenClassification\n",
    "\n",
    "pretrain_path = 'IDEA-CCNL/Erlangshen-ZEN1-224M-Chinese'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_path)\n",
    "model_classification = ZenForSequenceClassification.from_pretrained(pretrain_path)\n",
    "model_extraction = ZenForTokenClassification.from_pretrained(pretrain_path)\n",
    "ngram_dict = ZenNgramDict.from_pretrained(pretrain_path, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbacca91-e692-428f-9399-aa75b06e5553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cached_path' from 'transformers' (/home/sha/work/_p310/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzen2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mngram_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenNgramDict\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzen2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzen2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenForSequenceClassification, ZenForTokenClassification\n",
      "File \u001b[0;32m~/src/Fengshenbang-LM/fengshen/models/zen2/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_zen2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenForPreTraining, ZenForTokenClassification, ZenForSequenceClassification, ZenForQuestionAnswering, ZenModel, ZenForMaskedLM\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BasicTokenizer, WordpieceTokenizer, _is_whitespace, whitespace_tokenize, convert_to_unicode, _is_punctuation, _is_control, VOCAB_NAME\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mngram_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZenNgramDict, NGRAM_DICT_NAME, extract_ngram_feature, construct_ngram_matrix\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZenConfig\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZenForPreTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZenForTokenClassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZenForSequenceClassification\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZenForQuestionAnswering\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZenModel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZenForMaskedLM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBasicTokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract_ngram_feature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstruct_ngram_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n",
      "File \u001b[0;32m~/src/Fengshenbang-LM/fengshen/models/zen2/tokenization.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_path\n\u001b[1;32m     33\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     35\u001b[0m PRETRAINED_VOCAB_ARCHIVE_MAP \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-large-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIDEA-CCNL/Erlangshen-ZEN2-668M-Chinese\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/IDEA-CCNL/Erlangshen-ZEN2-668M-Chinese/resolve/main/vocab.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     51\u001b[0m }\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_path' from 'transformers' (/home/sha/work/_p310/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from fengshen.models.zen2.ngram_utils import ZenNgramDict\n",
    "from fengshen.models.zen2.tokenization import BertTokenizer\n",
    "from fengshen.models.zen2.modeling import ZenForSequenceClassification, ZenForTokenClassification\n",
    "\n",
    "pretrain_path = 'IDEA-CCNL/Erlangshen-ZEN2-668M-Chinese'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_path)\n",
    "model_classification = ZenForSequenceClassification.from_pretrained(pretrain_path)\n",
    "model_extraction = ZenForTokenClassification.from_pretrained(pretrain_path)\n",
    "ngram_dict = ZenNgramDict.from_pretrained(pretrain_path, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad35ffde-d46d-47f2-b2b4-88cda7dddbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############ 闻仲 ##################\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('IDEA-CCNL/Wenzhong-GPT2-3.5B')\n",
    "model = GPT2Model.from_pretrained('IDEA-CCNL/Wenzhong-GPT2-3.5B')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "029c81e9-3819-45ff-8fad-fda1a95af4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next sentence 0:\n",
      " 北京是中国的城市。在那里，谁也不知道哪里来的民众，哪里去接受苦难。在那里，每个人都可以感受到苦难。在那里，人们可以做到快乐，做到和平。在那里\n",
      "****************************************\n",
      "next sentence 1:\n",
      " 北京是中国的一个城市，但是，在那个年代，这个城市是一个很特殊的地方，是一个很穷的地方，这个地方就是北京，北京这个地方就是河北省的一个省级行政区，河北省的一个\n",
      "****************************************\n",
      "next sentence 2:\n",
      " 北京是中国的西北大省，曾是第一个进入北京的城市。曾经有人说，北京人的生活是很美好的，这是因为北京的人更加爱护生态环境。但是，我们的生活环境不好，空气污\n",
      "****************************************\n",
      "next sentence 3:\n",
      " 北京是中国的首都，在这里可以看到许多有着悠久历史的文化遗产，如北京故宫、北京博物院、北京钟楼等，这些都是中国历史文化名城。中国最美的城市，就是北京�\n",
      "****************************************\n",
      "next sentence 4:\n",
      " 北京是中国的一个城市，这里有着美丽的江南水乡风光，江南的水乡人文景观，江南的美食。首先我们来看看热爱中国的人们，这里的美食都是有名的，比如说山东�\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "hf_model_path = 'IDEA-CCNL/Wenzhong-GPT2-110M'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(hf_model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(hf_model_path)\n",
    "question = \"北京是中国的\"\n",
    "inputs = tokenizer(question,return_tensors='pt')\n",
    "generation_output = model.generate(**inputs,\n",
    "                                return_dict_in_generate=True,\n",
    "                                output_scores=True,\n",
    "                                max_length=150,\n",
    "                                # max_new_tokens=80,\n",
    "                                do_sample=True,\n",
    "                                top_p = 0.6,\n",
    "                                # num_beams=5,\n",
    "                                eos_token_id=50256,\n",
    "                                pad_token_id=0,\n",
    "                                num_return_sequences = 5)\n",
    "\n",
    "for idx,sentence in enumerate(generation_output.sequences):\n",
    "    print('next sentence %d:\\n'%idx,\n",
    "    tokenizer.decode(sentence).split('<|endoftext|>')[0])\n",
    "    print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff4a141-a287-4024-8cb8-b13f595a8436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1948b7929940eab3f004a6a4788806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '北京位于北纬39度，东经116度，'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese')\n",
    "model = GPT2Model.from_pretrained('IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "set_seed(55)\n",
    "generator = pipeline('text-generation', model='IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese')\n",
    "generator(\"北京位于\", max_length=30, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e228b-d0f3-480e-ad20-01193f770156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
