{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b1f3b9-d93f-4306-9364-ede4edb8c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62b691f-f147-4ffa-a4f5-71b08b073152",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb783026-f1d1-4213-8073-8a18505426eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(100, 100) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a881ccb8-89a9-47e8-9657-5d9467398055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.keras_tensor.KerasTensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0953c4df-5985-4ab4-9298-fd13a62554c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_type_spec': TensorSpec(shape=(100, 100), dtype=tf.float32, name='input_1'),\n",
       " '_inferred_value': None,\n",
       " '_name': 'input_1',\n",
       " '_keras_mask': None,\n",
       " '_keras_history': KerasHistory(layer=<keras.engine.input_layer.InputLayer object at 0x000002A886105990>, node_index=0, tensor_index=0)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cec9ba73-1e72-4c57-a4f5-9839876fb12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = layers.Dense(units=10, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6550aa2f-4b4c-4adb-9aa9-5767d1eec1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x2a885d62a10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07833b09-0554-4d30-bc13-a21782cf08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev\\_p310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at IDEA-CCNL/Yuyuan-GPT2-3.5B were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('IDEA-CCNL/Yuyuan-GPT2-3.5B')\n",
    "model = GPT2Model.from_pretrained('IDEA-CCNL/Yuyuan-GPT2-3.5B')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b419fe-1f17-4265-b6ea-a23efeb402d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adf996c89a34276bde4f1d736394b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff38cb4e7e04ce28788f87be6e02714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f69e57a7f14b4482f85367f5aaf63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424d55e8a9e84e18bc31b63aa4396fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361381d20385432eb9fa1877e0e88d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6a6ffca4484a7baa698f0df77549a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/279M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev\\work\\_p310\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Influenza is a highly contagious disease.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('IDEA-CCNL/Yuyuan-Bart-139M')\n",
    "model = BartForConditionalGeneration.from_pretrained('IDEA-CCNL/Yuyuan-Bart-139M')\n",
    "\n",
    "text = 'Influenza is a <mask> disease.'\n",
    "input_ids = tokenizer([text], return_tensors=\"pt\")['input_ids']\n",
    "model.eval()\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    ")\n",
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf76d023-9af5-4183-b2b2-35037522d554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a25231f6c64b51a19e8ecc944b9e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c9c64a5fbb45ee9e614f360a6544f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31490897a45c4592b893e60c138f24e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6f9c2851524bd0ba0aafee23a3d0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee5d7efc0734c4689707a6993d51d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd6ac89097a49fb92aebe2103db1eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/813M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Influenza is a highly contagious respiratory disease.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('IDEA-CCNL/Yuyuan-Bart-400M')\n",
    "model = BartForConditionalGeneration.from_pretrained('IDEA-CCNL/Yuyuan-Bart-400M')\n",
    "\n",
    "text = 'Influenza is a <mask> disease.'\n",
    "input_ids = tokenizer([text], return_tensors=\"pt\")['input_ids']\n",
    "model.eval()\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    ")\n",
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37341025-4701-4bd9-80bc-ffde531df572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at IDEA-CCNL/Yuyuan-GPT2-3.5B were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('IDEA-CCNL/Yuyuan-GPT2-3.5B')\n",
    "model = GPT2Model.from_pretrained('IDEA-CCNL/Yuyuan-GPT2-3.5B')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61122919-4994-402e-a99b-8e61a47d4658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\sha\\AppData\\Local\\Temp\\ipykernel_2280\\162144880.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\sha\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_2280\\\\162144880.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'image_data'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\sha\\AppData\\Local\\Temp\\ipykernel_2280\\162144880.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92m<module>\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\sha\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_2280\\\\162144880.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'image_data'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import clip\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "query_texts = [\"一只猫\", \"一只狗\",'两只猫', '两只老虎','一只老虎']  # 这里是输入文本的，可以随意替换。\n",
    "# 加载Taiyi 中文 text encoder\n",
    "text_tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\")\n",
    "text_encoder = BertForSequenceClassification.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\").eval()\n",
    "text = text_tokenizer(query_texts, return_tensors='pt', padding=True)['input_ids']\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # 这里可以换成任意图片的url\n",
    "# 加载CLIP的image encoder\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")  \n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "image = processor(images=Image.open(requests.get(url, stream=True).raw), return_tensors=\"pt\")\n",
    "if image_data.mode != 'RGB':\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**image)\n",
    "    text_features = text_encoder(text).logits\n",
    "    # 归一化\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    # 计算余弦相似度 logit_scale是尺度系数\n",
    "    logit_scale = clip_model.logit_scale.exp()\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    print(np.around(probs, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356bf599-469f-4f4c-9a62-444808c15f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f02666182a43259f11637c9746e632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f155e8fbb94adb8a8e23ee7296169f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e4f4f0a2bb477394a9cbf39de4928f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24cc70b5cc041bdadbd25f64e1560f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb9ff7a2712460ea0983ca037c0a64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02614af298ab4d3daa399d152162a2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"IDEA-CCNL/Taiyi-Roberta-124M-D-v2\")\n",
    "model = RobertaModel.from_pretrained(\"IDEA-CCNL/Taiyi-Roberta-124M-D-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c0a9e4-b98b-4c3f-b76a-871b46555600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe6ce4aed2a4385825cdbf0c55faf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a473a178d7c3461693c97880cfc1543b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb032c7918c47e0b21ce74b07a8708d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e8dc47e68b4b4c9825954989150724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948a443f91d348b7b573655b84011b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8452f08e864950befacdcbcd062900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"IDEA-CCNL/Taiyi-Roberta-124M-D\")\n",
    "model = RobertaModel.from_pretrained(\"IDEA-CCNL/Taiyi-Roberta-124M-D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "386cedef-40d9-4d6c-8358-768a88228a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a685f783c9094c6cb7e868b0c550d068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev\\work\\_p310\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bf449a11b6472bb491436a51c707d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca643c0a3184ff3bcb04c3887a81a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('IDEA-CCNL/Taiyi-vit-87M-D')\n",
    "model = ViTForImageClassification.from_pretrained('IDEA-CCNL/Taiyi-vit-87M-D')\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
    "# Predicted class: Egyptian cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "995020f4-a0b9-4aea-80c1-20aa47a69d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab300dcf6054d759dcf44553557614d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/419 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3745c2f8137b45659048e52a882297fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"spiece.model\";:   0%|          | 0.00/859k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eaa3f03a7f041cc90c49761ac1a7cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e39db79cf646838ced696a61e4626c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/33.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5fee1dec4f4c21a3dc9442a705e0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/157 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbea307cad34c9eb3ae3a669a66136d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bf24a4a8d74f90820a733c36a5553a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/279M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '谷爱凌获自由式滑雪女子坡面障碍技巧银牌'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer, Text2TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "model=BartForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "text = 'summary:在北京冬奥会自由式滑雪女子坡面障碍技巧决赛中，中国选手谷爱凌夺得银牌。祝贺谷爱凌！今天上午，自由式滑雪女子坡面障碍技巧决赛举行。决赛分三轮进行，取选手最佳成绩排名决出奖牌。第一跳，中国选手谷爱凌获得69.90分。在12位选手中排名第三。完成动作后，谷爱凌又扮了个鬼脸，甚是可爱。第二轮中，谷爱凌在道具区第三个障碍处失误，落地时摔倒。获得16.98分。网友：摔倒了也没关系，继续加油！在第二跳失误摔倒的情况下，谷爱凌顶住压力，第三跳稳稳发挥，流畅落地！获得86.23分！此轮比赛，共12位选手参赛，谷爱凌第10位出场。网友：看比赛时我比谷爱凌紧张，加油！'\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)\n",
    "print(text2text_generator(text, max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29fdc44-c8a7-4feb-b530-ee9cff4f7ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eac976985a42ce9f7b1de27f98fe20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/279M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '桂林市是世界闻名遐迩的旅游城市 ,它有悠久的旅游历史。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer, Text2TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M', use_fast=False)\n",
    "model=BartForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-BART-139M')\n",
    "text = '<s>桂林市是世界闻名<mask> ，它有悠久的<mask>'\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)\n",
    "print(text2text_generator(text, max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea3cc1c-2fc9-45e4-80d0-04cf1c1c52f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71981f4caad4a10b64cef7355ed9ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '桂 林 是 著 名 的 旅 游 城 市 ， 它 有 很 多 景 点 。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer, Text2TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-759M-Chinese-BertTokenizer', use_fast=False)\n",
    "model=BartForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-BART-759M-Chinese-BertTokenizer')\n",
    "text = '桂林是著名的[MASK]，它有很多[MASK]。'\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)\n",
    "print(text2text_generator(text, max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0778da5-7189-4812-8da7-3bb1e3a563dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fengshen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfengshen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepVAE\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep_vae\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Della\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_bert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDEA-CCNL/Randeng-DELLA-226M-Chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fengshen'"
     ]
    }
   ],
   "source": [
    "# Checkout the latest Fengshenbang-LM directory and run following script under Fengshenbang-LM root directory \n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from fengshen.models.deepVAE.deep_vae import Della\n",
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-DELLA-226M-Chinese\")\n",
    "vae_model = Della.from_pretrained(\"IDEA-CCNL/Randeng-DELLA-226M-Chinese\")\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "sentence =  \"本模型是在通用数据集下预训练的VAE模型，如要获得最佳效果请在特定领域微调后使用。\"\n",
    "tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))\n",
    "decoder_target = [tokenizer.bos_token_id] + tokenized_text + [tokenizer.eos_token_id]\n",
    "inputs = []\n",
    "inputs.append(torch.tensor(decoder_target, dtype=torch.long))\n",
    "inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "max_length = 256\n",
    "top_p = 0.5\n",
    "top_k = 0\n",
    "temperature = .7\n",
    "repetition_penalty = 1.0\n",
    "sample = False\n",
    "device = 0\n",
    "model = vae_model.eval()\n",
    "model = model.to(device)\n",
    "outputs = model.model.inference(inputs.to(device), top_p=top_p, top_k=top_k, max_length=max_length, sample=sample,\n",
    "    temperature=temperature, repetition_penalty=repetition_penalty)\n",
    "for gen_sent, orig_sent in zip(outputs, inputs):\n",
    "    print('orig_sent:', tokenizer.decode(orig_sent).replace(' ', ''))\n",
    "    print('gen_sent:', tokenizer.decode(gen_sent).replace(' ', ''))\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c89b4b-ebc3-44ed-a664-f2077a2f5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fengshen.models.DAVAE.DAVAEModel import DAVAEModel\n",
    "from transformers import BertTokenizer,T5Tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder_tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-DAVAE-1.2B-General-Chinese\")\n",
    "decoder_tokenizer = T5Tokenizer.from_pretrained(\"IDEA-CCNL/Randeng-DAVAE-1.2B-General-Chinese\", eos_token = '<|endoftext|>', pad_token = '<pad>',extra_ids=0)\n",
    "decoder_tokenizer.add_special_tokens({'bos_token':'<bos>'})\n",
    "vae_model = DAVAEModel.from_pretrained(\"IDEA-CCNL/Randeng-DAVAE-1.2B-General-Chinese\").to(device)\n",
    "input_texts = [\n",
    "    \"针对电力系统中的混沌振荡对整个互联电网的危害问题,提出了一种基于非线性光滑函数的滑模控制方法.\",\n",
    "    \"超市面积不算大.挺方便附近的居民购买的. 生活用品也比较齐全.价格适用中.\",\n",
    "]\n",
    "output_texts = vae_model.simulate_batch(encoder_tokenizer,decoder_tokenizer,input_texts)\n",
    "print(output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fa42e-fce6-42ec-a316-868e469a630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer,T5Tokenizer\n",
    "from fengshen.models.GAVAE.GAVAEModel import GAVAEModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_texts = [\n",
    "    \"非常好的一个博物馆，是我所有去过的博物馆里感觉最正规的一家，凭有效证件可以入馆，可以自助免费存小件物品，讲解员和馆内外的工作人员也非常认真，其他的服务人员也很热情，非常好的！馆内的藏品也让人非常震撼！希望继续保持～\", \n",
    "    \"这是我来长沙最最期待的一定要去的地方，总算今天特地去瞻仰千古遗容了，开车到门口大屏幕显示着门票已发完的字样，心里一惊以为今天是白来了。但进了停车场才知道凭停车卡和有效身份证里面也能领，停车还不花钱，真好。\", \n",
    "    \"地方很大 很气派~~可以逛很久~~~去的时候是免费的~不过要安检~~~里面的马王堆~幸追夫人~还是很不错的~~~~去的时候有一个吴越文化特别展~~~东西也很多~~~~~很好看\",\n",
    "    \"我们到达的时候是下午3点，门票已经发完了。当时正焦虑的不知道怎么办才好，门卫大哥给我们俩补办了门票，这才得以入馆。非常感谢！绝对不虚此行！相当震撼的展览！原来古人也化妆，还有假发。记忆最深的是那个藕汤。可惜真颜已不得见。\", \n",
    "    \"去过三次，个人认为这是长沙最值得去的地方，博物馆的重点就是辛追，遗憾的是，每次去我都会感到悲哀，虽然我三次去的时候都要门票，但是每次看到辛追，都觉得现代的人类不应该挖她出来，除了第一次我觉得辛追像刚死去一样，后来两次我觉得太惨不忍睹了。建议大家要去就早去，以后肯定越来越腐烂\", \n",
    "    \"上大学时候去的，当时学生证是半价25，后来凭有效证件就不要钱了。非常喜欢的一家博物馆，里面可看的东西很多，当然最吸引我的就是那个辛追夫人和“素纱单衣”，果然不是盖的~里面的讲解员大部分都是师大学历史类的，非常专业和有耐心。虽然不在长沙了，不过对那里还是很有感情的，赞~~~\", \n",
    "    \"这两年也有很多机会去博物馆。。。不过还是想说湖南省博物馆是非常有特色的。。。应该说整个展览分成两个部分吧。。。一个部分是马王堆的主体展。。。另一个就是湖南的一些考古发现。。。其实来省博大部分的游客还是冲着马王堆来的吧。。。博物馆也很有心的为每一批游客安排了讲解员。。。从马王堆的发现到马王堆出土文物的介绍再到最后棺木和辛追的介绍。。。真是上了一节很生动的历史课。\",\n",
    "    \"网上订票去的，还是很顺利的就进去了，里面挺清净的，外围的环境也不错，还有鸽子可以喂。那天不是很闹，兜了一圈感觉还是很顺畅的，老娘娘和金缕玉衣挺震撼的。到此一游还是挺需要的\",\n",
    "]\n",
    "encoder_tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-GAVAE-1.2B-Augmentation-Chinese\")\n",
    "decoder_tokenizer = T5Tokenizer.from_pretrained(\"IDEA-CCNL/Randeng-GAVAE-1.2B-Augmentation-Chinese\", eos_token = '<|endoftext|>', pad_token = '<pad>',extra_ids=0)\n",
    "decoder_tokenizer.add_special_tokens({'bos_token':'<bos>'})\n",
    "gavae_model = GAVAEModel.from_pretrained(\"IDEA-CCNL/Randeng-GAVAE-1.2B-Augmentation-Chinese\").to(device)\n",
    "gavae_model.train_gan(encoder_tokenizer,decoder_tokenizer,input_texts)\n",
    "# n:输出样本数量\n",
    "texts = gavae_model.generate(n=5)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413cccb-8cdf-4896-8b9d-1d6aa641314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fengshen import T5ForConditionalGeneration\n",
    "from fengshen import T5Config\n",
    "from fengshen import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('IDEA-CCNL/Randeng-MegatronT5-770M')\n",
    "config = T5Config.from_pretrained('IDEA-CCNL/Randeng-MegatronT5-770M')\n",
    "model = T5ForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-MegatronT5-770M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71d62b-6312-4b8f-998c-4933eb14e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration\n",
    "# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n",
    "# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_238M/tree/main\n",
    "# Stronly recomend you git clone the Fengshenbang-LM repo:\n",
    "# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n",
    "# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n",
    "# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n",
    "from tokenizers_pegasus import PegasusTokenizer\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Chinese\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Chinese\")\n",
    "\n",
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "inputs = tokenizer(text, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "# model output: 截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73ad03-fa0d-4bf5-8cba-d4813078557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration,BertTokenizer\n",
    "# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n",
    "# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\n",
    "# Strongly recommend you git clone the Fengshenbang-LM repo:\n",
    "# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n",
    "# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n",
    "# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n",
    "\n",
    "from tokenizers_pegasus import PegasusTokenizer\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese\")\n",
    "\n",
    "text = \"在北京冬奥会自由式滑雪女子坡面障碍技巧决赛中，中国选手谷爱凌夺得银牌。祝贺谷爱凌！今天上午，自由式滑雪女子坡面障碍技巧决赛举行。决赛分三轮进行，取选手最佳成绩排名决出奖牌。第一跳，中国选手谷爱凌获得69.90分。在12位选手中排名第三。完成动作后，谷爱凌又扮了个鬼脸，甚是可爱。第二轮中，谷爱凌在道具区第三个障碍处失误，落地时摔倒。获得16.98分。网友：摔倒了也没关系，继续加油！在第二跳失误摔倒的情况下，谷爱凌顶住压力，第三跳稳稳发挥，流畅落地！获得86.23分！此轮比赛，共12位选手参赛，谷爱凌第10位出场。网友：看比赛时我比谷爱凌紧张，加油！\"\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# model Output: 滑雪女子坡面障碍技巧决赛谷爱凌获银牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7934ef-c247-4bec-8da4-a356975092dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration\n",
    "# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n",
    "# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\n",
    "# Strongly recommend you git clone the Fengshenbang-LM repo:\n",
    "# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n",
    "# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n",
    "# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n",
    "from tokenizers_pegasus import PegasusTokenizer\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\")\n",
    "\n",
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# model Output: 反垄断调查小组突击查访奔驰上海办事处，对多名奔驰高管进行约谈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ea0d2-275c-4cea-81a4-37abbd9f3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration\n",
    "# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n",
    "# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\n",
    "# Strongly recommend you git clone the Fengshenbang-LM repo:\n",
    "# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n",
    "# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n",
    "# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n",
    "from tokenizers_pegasus import PegasusTokenizer\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\")\n",
    "\n",
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# model Output: 截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
